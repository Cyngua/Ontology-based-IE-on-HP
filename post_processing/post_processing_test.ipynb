{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuned Models Post-processing\n",
    "\n",
    "* Minimal: Decode model outputs back to text and semantic tags (merge B/I, skip O)\n",
    "* Good to have: Extract word-tag pairs\n",
    "* Best to have: Map words to SNOMED CT terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# torch essentials\n",
    "import torch\n",
    "from transformers import BertTokenizerFast, BertConfig, BertForTokenClassification\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from seqeval.metrics import classification_report\n",
    "from transformers import EvalPrediction\n",
    "from seqeval.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-defined utils\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"../utils\"))\n",
    "from bert_dataset import BERTDataset\n",
    "from deberta_dataset import DeBERTaDataset\n",
    "from prepare_dataset import prepare_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT (Finetuned)\n",
    "\n",
    "Training Arguments\n",
    "```\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=6,\n",
    "    learning_rate=2e-5, # smaller learning rate\n",
    "    seed=42, # for deterministic results\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    fp16=True, # mixed precision training\n",
    "    report_to=\"none\",\n",
    ")\n",
    "```\n",
    "\n",
    "Performance:\n",
    "|               | Accuracy | Precision | Recall | F1     | Validation Loss | Train Runtime |\n",
    "|---------------|----------|-----------|--------|--------|-----------------|---------------|\n",
    "| BERT          | 0.9288   | 0.7144    | 0.7751 | 0.7435 | 0.2834          | 1239.1881     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "MAX_LEN = 128\n",
    "DATA_PATH = \"../processed_notes.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Load Fine-tuned model and Tokenier\n",
    "model_path = \"../saved_bert_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_path,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (20305, 4)\n",
      "TRAIN Dataset: (16244, 4)\n",
      "TEST Dataset: (4061, 4)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load Dataset\n",
    "data_dict = prepare_data(\"../processed_notes.csv\")\n",
    "data = data_dict[\"data\"]\n",
    "labels_to_ids = data_dict[\"labels_to_ids\"]\n",
    "ids_to_labels = data_dict[\"ids_to_labels\"]\n",
    "\n",
    "NUM_LABELS = len(labels_to_ids)\n",
    "train_size = 0.8\n",
    "train_dataset = data.sample(frac=train_size,random_state=200)\n",
    "test_dataset = data.drop(train_dataset.index).reset_index(drop=True)\n",
    "print(\"FULL Dataset: {}\".format(data.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "# we only evaluate on the testing set here\n",
    "testing_set = BERTDataset(test_dataset, tokenizer, MAX_LEN, labels_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p: EvalPrediction):\n",
    "    predictions = p.predictions\n",
    "    labels = p.label_ids\n",
    "\n",
    "    # Get the class with highest probability for each token\n",
    "    predicted_ids = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    true_labels = []\n",
    "    true_predictions = []\n",
    "\n",
    "    for pred, label in zip(predicted_ids, labels):\n",
    "        curr_preds = []\n",
    "        curr_labels = []\n",
    "        for p_id, l_id in zip(pred, label):\n",
    "            if l_id != -100:\n",
    "                curr_preds.append(ids_to_labels[p_id])  # predicted label string\n",
    "                curr_labels.append(ids_to_labels[l_id])  # true label string\n",
    "        true_predictions.append(curr_preds)\n",
    "        true_labels.append(curr_labels)\n",
    "    print(classification_report(true_labels, true_predictions))\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(true_labels, true_predictions),\n",
    "        \"precision\": precision_score(true_labels, true_predictions),\n",
    "        \"recall\": recall_score(true_labels, true_predictions),\n",
    "        \"f1\": f1_score(true_labels, true_predictions)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ch/1v21vldj0m1dk5lxd7kjbrh80000gn/T/ipykernel_17334/40364536.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='254' max='254' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [254/254 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ACTI       0.67      0.59      0.62        17\n",
      "        ANAT       0.63      0.72      0.67       755\n",
      "        CHEM       1.00      0.75      0.86         4\n",
      "        CONC       0.54      0.56      0.55        66\n",
      "        DISO       0.76      0.82      0.79      5023\n",
      "        OBJC       0.67      0.50      0.57         4\n",
      "        PHEN       0.50      0.04      0.08        23\n",
      "        PHYS       0.83      0.73      0.77        85\n",
      "        PROC       0.63      0.71      0.67      1938\n",
      "         UNK       0.35      0.30      0.32        27\n",
      "\n",
      "   micro avg       0.71      0.78      0.74      7942\n",
      "   macro avg       0.66      0.57      0.59      7942\n",
      "weighted avg       0.72      0.78      0.74      7942\n",
      "\n",
      "{'eval_loss': 0.28257325291633606, 'eval_model_preparation_time': 0.0016, 'eval_accuracy': 0.9288112929449485, 'eval_precision': 0.7143354210160056, 'eval_recall': 0.7754973558297658, 'eval_f1': 0.7436609514609998, 'eval_runtime': 30.649, 'eval_samples_per_second': 132.5, 'eval_steps_per_second': 8.287}\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Evaluation on the validation set\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_eval_batch_size=16,\n",
    "    do_eval=True,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(testing_set)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inject into model config\n",
    "model.config.id2label = {i: ids_to_labels[i] for i in ids_to_labels}\n",
    "model.config.label2id = {v: k for k, v in model.config.id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Decoding Pipeline\n",
    "ner_pipe = pipeline(\n",
    "    \"ner\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\",  # combines B/I tokens into full spans\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity                         Tag        Score \n",
      "--------------------------------------------------\n",
      "dialysis                       PROC       0.95\n",
      "ur                             DISO       1.00\n",
      "##emia                         DISO       0.86\n",
      "severe                         DISO       0.95\n",
      "hyperkalemia                   DISO       0.99\n",
      "refrac                         DISO       0.99\n",
      "##tory metabolic acidosis      DISO       0.99\n",
      "dia                            DISO       1.00\n",
      "##rr                           DISO       1.00\n",
      "##hea                          DISO       0.65\n",
      "gout                           DISO       0.89\n",
      "diuretics                      PROC       0.85\n",
      "monitor creatinine             PROC       0.86\n",
      "hemodialysis                   PROC       0.96\n",
      "consultation                   PROC       0.69\n"
     ]
    }
   ],
   "source": [
    "predictions = ner_pipe('''\n",
    "    No urgent indications of dialysis including uremia, persistent severe hyperkalemia, refractory metabolic acidosis.  However patient is extremely hypervolemic \n",
    "\n",
    "    Triggers are likely diarrhea and recent starting of colchicine for gout.   \n",
    "\n",
    "    Hold off on diuretics  \n",
    "\n",
    "    Monitor creatinine closely  \n",
    "\n",
    "    Patient is willing for hemodialysis.  We will put Eliquis on hold, plan for PermCath and nephrology consultation  \n",
    "    ''')\n",
    "# Print nicely aligned output\n",
    "print(f\"{'Entity':<30} {'Tag':<10} {'Score':<6}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for ent in predictions:\n",
    "    if ent[\"entity_group\"] != \"O\":  # Optional: filter out \"O\"\n",
    "        print(f\"{ent['word']:<30} {ent['entity_group']:<10} {ent['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity                         Tag        Score \n",
      "--------------------------------------------------\n",
      "dia                            DISO       1.00\n",
      "##rrhea                        DISO       1.00\n",
      "liquid stools                  DISO       0.98\n",
      "incontinence                   DISO       0.98\n",
      "burning in the urine           DISO       1.00\n",
      "medications                    PROC       0.57\n"
     ]
    }
   ],
   "source": [
    "predictions = ner_pipe('''\n",
    "        Patient reported that he has been having diarrhea which she described as liquid stools with incontinence.  \n",
    "        He also reported burning in the urine.  \n",
    "        He has been taking all of his medications including Bumex, colchicine \n",
    "        ''')\n",
    "# Print nicely aligned output\n",
    "print(f\"{'Entity':<30} {'Tag':<10} {'Score':<6}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for ent in predictions:\n",
    "    if ent[\"entity_group\"] != \"O\":  # Optional: filter out \"O\"\n",
    "        print(f\"{ent['word']:<30} {ent['entity_group']:<10} {ent['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clinical BERT (Finetuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Load Fine-tuned model and Tokenier\n",
    "model_path = \"../saved_clinicalbert_model_epoch6\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_path,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset is the same as BERT\n",
    "testing_set = BERTDataset(test_dataset, tokenizer, MAX_LEN, labels_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ch/1v21vldj0m1dk5lxd7kjbrh80000gn/T/ipykernel_17334/40364536.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='254' max='254' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [254/254 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ACTI       0.69      0.53      0.60        17\n",
      "        ANAT       0.64      0.71      0.67       750\n",
      "        CHEM       1.00      0.75      0.86         4\n",
      "        CONC       0.59      0.58      0.58        64\n",
      "        DISO       0.76      0.82      0.79      4965\n",
      "        OBJC       0.67      0.50      0.57         4\n",
      "        PHEN       0.09      0.04      0.06        23\n",
      "        PHYS       0.80      0.71      0.75        84\n",
      "        PROC       0.66      0.72      0.69      1917\n",
      "         UNK       0.50      0.19      0.27        27\n",
      "\n",
      "   micro avg       0.72      0.78      0.75      7855\n",
      "   macro avg       0.64      0.55      0.58      7855\n",
      "weighted avg       0.72      0.78      0.75      7855\n",
      "\n",
      "{'eval_loss': 0.25763267278671265, 'eval_model_preparation_time': 0.0007, 'eval_accuracy': 0.930499335543098, 'eval_precision': 0.7231718898385565, 'eval_recall': 0.7755569700827498, 'eval_f1': 0.7484489219239511, 'eval_runtime': 18.3468, 'eval_samples_per_second': 221.346, 'eval_steps_per_second': 13.844}\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Evaluation on the validation set\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_eval_batch_size=16,\n",
    "    do_eval=True,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate(testing_set)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clinical-ner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
